{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"numpy<2\"\n",
    "%pip install transformers\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd53a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25232e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 設定 ---\n",
    "ROOT_DIR = \"/workspace/wm/datasets\"\n",
    "TASK = \"pusht_noise\"\n",
    "TRAIN_VAL = \"val\" # \"train\" or \"val\"\n",
    "OVERWRITE_JSONL = False\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "PROMPT = (\n",
    "    \"Given this image as a goal state for a PushT robot task, \"\n",
    "    \"provide a concise caption between 3 to 8 words \"\n",
    "    \"that implies the achieved state.\"\n",
    ")\n",
    "\n",
    "VIDEO_DIR = f\"{ROOT_DIR}/{TASK}/{TRAIN_VAL}/obses\"\n",
    "LAST_FRAME_DIR = f\"{ROOT_DIR}/{TASK}/{TRAIN_VAL}/last_frames\"\n",
    "OUTPUT_JSONL = f\"{ROOT_DIR}/{TASK}/{TRAIN_VAL}/captions.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- データセット(mp4)の最後のフレームを取り出す ---\n",
    "\n",
    "def save_last_frames(video_dir, output_dir):\n",
    "    \"\"\"\n",
    "    動画ディレクトリから各動画の最終フレームを抽出し保存する。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    video_extensions = ['.mp4']\n",
    "    video_files = [f for f in os.listdir(video_dir) if Path(f).suffix.lower() in video_extensions]\n",
    "    video_files.sort()\n",
    "    \n",
    "    print(f\"Found {len(video_files)} videos in {video_dir}\")\n",
    "    \n",
    "    for video_name in tqdm(video_files, desc=\"Extracting\"):\n",
    "        video_path = os.path.join(video_dir, video_name)\n",
    "        output_path = os.path.join(output_dir, f\"{Path(video_name).stem}_last.jpg\")\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            # すでに画像がある場合はスキップ\n",
    "            continue\n",
    "            \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames > 0:\n",
    "            # 最終フレームへシーク\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)\n",
    "            success, frame = cap.read()\n",
    "            if success:\n",
    "                cv2.imwrite(output_path, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "            else:\n",
    "                print(f\"Warning: Could not read the last frame of {video_name}\")\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_last_frames(VIDEO_DIR, LAST_FRAME_DIR)\n",
    "    print(f\"\\n✓ Process completed. Frames saved to: {LAST_FRAME_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 最後のフレームとプロンプトをVLMに渡して、キャプションをつける ---\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    モデルとプロセッサをロードし、GPUに配置する\n",
    "    \"\"\"\n",
    "    # 4bit量子化の設定（VRAM節約）\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    print(f\"Loading model: {MODEL_ID}\")\n",
    "    model = MllamaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # quantization_config=bnb_config, # 量子化\n",
    "        device_map={\"\": 0}, # \"auto\"だと、自動的に最適なデバイス（GPU/CPU）へ割り当て。ただし、\"auto\"の方が推論遅いかも?\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "    return model, processor\n",
    "\n",
    "def generate_captions():\n",
    "    if os.path.exists(OUTPUT_JSONL) and not OVERWRITE_JSONL:\n",
    "        print(f\"Skipping: {OUTPUT_JSONL} already exists.\")\n",
    "        return\n",
    "\n",
    "    # ディレクトリの準備\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSONL), exist_ok=True)\n",
    "    \n",
    "    # モデルの準備\n",
    "    model, processor = load_model()\n",
    "\n",
    "    # 画像リストの準備\n",
    "    image_files = [f for f in os.listdir(LAST_FRAME_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    image_files.sort()\n",
    "\n",
    "    print(f\"Total images found: {len(image_files)}\")\n",
    "\n",
    "    # 推論ループ\n",
    "    with torch.no_grad():\n",
    "        with open(OUTPUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "            for image_name in tqdm(image_files, desc=\"Generating captions\"):\n",
    "                image_path = os.path.join(LAST_FRAME_DIR, image_name)\n",
    "                \n",
    "                try:\n",
    "                    # 画像の読み込み\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "                    # メッセージの構築\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                            {\"type\": \"image\"},\n",
    "                            {\"type\": \"text\", \"text\": PROMPT}\n",
    "                        ]}\n",
    "                    ]\n",
    "                    \n",
    "                    # 入力の準備\n",
    "                    input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "                    inputs = processor(\n",
    "                        image,\n",
    "                        input_text,\n",
    "                        add_special_tokens=False,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).to(model.device)\n",
    "\n",
    "                    # 推論実行\n",
    "                    output = model.generate(**inputs, max_new_tokens=30)\n",
    "                    \n",
    "                    # テキストのデコードと抽出\n",
    "                    full_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "                    caption = full_text.split(\"assistant\")[-1].strip()\n",
    "\n",
    "                    # 結果を保存\n",
    "                    result = {\n",
    "                        \"file_name\": image_name,\n",
    "                        \"caption\": caption\n",
    "                    }\n",
    "                    f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "                    f.flush()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing {image_name}: {e}\")\n",
    "                \n",
    "                finally:\n",
    "                    # メモリ解放\n",
    "                    if 'inputs' in locals(): del inputs\n",
    "                    if 'output' in locals(): del output\n",
    "                    if 'image' in locals(): del image\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_captions()\n",
    "    print(f\"\\n✓ 完了！ 結果は {OUTPUT_JSONL} に保存されました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
